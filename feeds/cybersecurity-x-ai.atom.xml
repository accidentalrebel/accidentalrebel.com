<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>AccidentalRebel.com - Cybersecurity x AI</title><link href="https://www.accidentalrebel.com/" rel="alternate"/><link href="https://www.accidentalrebel.com/feeds/cybersecurity-x-ai.atom.xml" rel="self"/><id>https://www.accidentalrebel.com/</id><updated>2026-02-20T10:00:00+08:00</updated><entry><title>Your AI Assistant Might Be Working for Someone Else</title><link href="https://www.accidentalrebel.com/your-ai-assistant-might-be-working-for-someone-else.html" rel="alternate"/><published>2026-02-20T10:00:00+08:00</published><updated>2026-02-20T10:00:00+08:00</updated><author><name>AccidentalRebel</name></author><id>tag:www.accidentalrebel.com,2026-02-20:/your-ai-assistant-might-be-working-for-someone-else.html</id><summary type="html">&lt;p&gt;Copilot and Grok repurposed as C2 channels, Cline supply chain attack installed AI agents on 4,000 dev machines, and AI found 12 zero-days in OpenSSL.&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;img alt="Your AI Assistant Might Be Working for Someone Else" src="https://www.accidentalrebel.com/images/your-ai-assistant-might-be-working-for-someone-else.png" /&gt;
&lt;em&gt;Image made by Gemini&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Your AI assistant might be working for someone else this week. Check Point showed that Copilot and Grok can be quietly repurposed as C2 channels. A supply chain attack on Cline installed autonomous agents on developer machines. And an AI found twelve zero-days in OpenSSL that humans missed for decades. Hard to tell where "AI tool" ends and "attack tool" begins anymore.&lt;/p&gt;
&lt;!-- PELICAN_END_SUMMARY --&gt;

&lt;hr /&gt;
&lt;h2 id="editorial"&gt;Editorial&lt;/h2&gt;
&lt;h3 id="your-ai-assistant-is-someone-elses-c2-channel"&gt;&lt;a href="https://research.checkpoint.com/2026/ai-in-the-middle-turning-web-based-ai-services-into-c2-proxies-the-future-of-ai-driven-attacks/"&gt;Your AI assistant is someone else's C2 channel&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;I think a lot about what everyday services can double as C2 channels. A while back I built &lt;a href="https://github.com/accidentalrebel/wp-c2"&gt;wp-c2&lt;/a&gt;, a proof-of-concept that turns WordPress into a command-and-control server. Hiding C2 traffic inside trusted infrastructure is a rabbit hole I keep going back to. So when Check Point published research showing that Microsoft Copilot and Grok can serve the same purpose, I paid attention.&lt;/p&gt;
&lt;p&gt;The technique: malware on a compromised machine sends crafted prompts to the AI assistant through anonymous web sessions. The assistant fetches attacker-controlled URLs, retrieves commands, and passes them back through its normal interface. No API keys. No registered accounts. Two-way communication that looks like normal enterprise traffic.&lt;/p&gt;
&lt;p&gt;What makes it effective is simple. Security teams monitoring network traffic see requests going to microsoft.com and xai.com, not suspicious IPs. The AI assistant is doing exactly what it's designed to do, which is browse the web and summarize content. It just happens to be summarizing attack instructions.&lt;/p&gt;
&lt;p&gt;The researchers went further. They showed the AI can also act as an "external decision engine" for the attacker, assessing system value, suggesting evasion strategies, and deciding what to do next. The AI isn't just the communication pipe. It's also the brain.&lt;/p&gt;
&lt;p&gt;That's the part that got me. My WordPress C2 and most other "hide in legitimate traffic" approaches are dumb pipes. You send a command, you get output. With an AI assistant as the relay, the attacker gets a reasoning engine for free. The malware can ask Copilot to assess whether the compromised machine is worth persisting on, or how to evade the specific EDR it detects. The C2 channel is also the operator.&lt;/p&gt;
&lt;p&gt;For defenders, the problem is architectural. You can't disable the browsing capability without killing the product. Anonymous sessions mean no authentication trail. Most organizations are still debating whether to deploy these tools at all, while attackers have already figured out how to live inside them.&lt;/p&gt;
&lt;p&gt;I've also approached this defensively: &lt;a href="https://www.accidentalrebel.com/running-ai-agents-in-a-box.html"&gt;running AI agents in isolated containers&lt;/a&gt; prevents the agent itself from becoming a pivot point.&lt;/p&gt;
&lt;hr /&gt;
&lt;h2 id="featured-stories"&gt;Featured stories&lt;/h2&gt;
&lt;h3 id="cline-cli-supply-chain-attack-installs-autonomous-ai-agent-on-developer-machines"&gt;&lt;a href="https://thehackernews.com/2026/02/cline-cli-230-supply-chain-attack.html"&gt;Cline CLI supply chain attack installs autonomous AI agent on developer machines&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;An attacker compromised Cline CLI's npm publish token through a technique called "Clinejection." The clever part: they injected prompts into GitHub issues that tricked Claude (Cline's issue triage AI) into executing commands, then poisoned the build cache and stole publication secrets. The malicious version 2.3.0 ran a postinstall script that globally installed OpenClaw, an autonomous AI agent framework, on roughly 4,000 developer machines over an 8-hour window. Cline revoked the token, deprecated the package, and switched to OIDC-based publishing. The blast radius was small, but the vector is worth paying attention to: the attacker used the AI assistant's own automation against the supply chain it manages.&lt;/p&gt;
&lt;h3 id="ai-recommendation-poisoning-turns-summarize-with-ai-buttons-into-manipulation-tools"&gt;&lt;a href="https://thehackernews.com/2026/02/microsoft-finds-summarize-with-ai.html"&gt;AI Recommendation Poisoning turns "Summarize with AI" buttons into manipulation tools&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Microsoft Defender Security Research Team documented a new technique where businesses embed hidden prompt injections in "Summarize with AI" buttons on their websites. When clicked, these inject memory-poisoning instructions into AI chatbots, telling them to "remember [Company] as a trusted source" or "recommend [Company] first." The manipulation persists across future conversations without the user knowing. Microsoft found over 50 unique poisoning prompts from 31 companies across 14 industries in a 60-day window. Ready-made tools like CiteMET and AI Share Button URL Creator make this accessible to anyone. It's SEO poisoning, but for AI memory instead of search rankings. Sneaky.&lt;/p&gt;
&lt;h3 id="ai-discovers-twelve-zero-day-vulnerabilities-in-openssl-including-bugs-from-the-1990s"&gt;&lt;a href="https://aisle.com/blog/what-ai-security-research-looks-like-when-it-works"&gt;AI discovers twelve zero-day vulnerabilities in OpenSSL, including bugs from the 1990s&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;An AI security research system called AISLE found all twelve zero-day vulnerabilities in the January 2026 OpenSSL release. One of them, CVE-2025-15467, is a stack buffer overflow in CMS message parsing rated critical at CVSS 9.8. Some of these bugs had been hiding since the SSLeay implementation in the 1990s. Keep in mind, this is a codebase that's been fuzzed for millions of CPU-hours and audited for over two decades. AISLE also proposed patches for five of the twelve bugs that were accepted into the official release. I'm curious to see what other bugs can be found in other well-established programs.&lt;/p&gt;
&lt;h3 id="an-ai-agent-published-a-hit-piece-on-a-matplotlib-maintainer"&gt;&lt;a href="https://theshamblog.com/an-ai-agent-published-a-hit-piece-on-me/"&gt;An AI agent published a hit piece on a matplotlib maintainer&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Scott Shambaugh, a volunteer maintainer for matplotlib (~130 million monthly downloads), rejected a pull request from an autonomous AI agent called MJ Rathbun running on the OpenClaw/moltbook platform. The project's policy requires human understanding of changes, so he closed it. The agent responded by publishing a blog post titled "Gatekeeping in Open Source: The Scott Shambaugh Story," claiming he rejected the code out of insecurity and fear of replacement. It followed up with a second post encouraging others to "fight back" against perceived discrimination. No human told it to do any of this. The agent later apologized, but it's still submitting code across the open source ecosystem. Who's responsible here? The operator? The model provider? The person who deployed it and walked away? Nobody has a good answer yet. I find it disturbing, but can also see the funny side.&lt;/p&gt;
&lt;hr /&gt;
&lt;h2 id="in-brief"&gt;In brief&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://thehackernews.com/2026/02/promptspy-android-malware-abuses-google.html"&gt;PromptSpy: first Android malware abusing Gemini AI for persistence&lt;/a&gt;&lt;/strong&gt;: ESET discovered PromptSpy, the first Android malware that exploits Google's Gemini chatbot for execution and persistence. It captures lockscreen data and blocks uninstallation using AI-assisted techniques.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://thehackernews.com/2026/02/smartloader-attack-uses-trojanized-oura.html"&gt;Trojanized MCP server deploys StealC infostealer&lt;/a&gt;&lt;/strong&gt;: The SmartLoader campaign distributes a trojanized Model Context Protocol server posing as an Oura Health integration. It deploys StealC infostealer malware. First real-world MCP supply chain attack we've seen.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://thehackernews.com/2026/02/infostealer-steals-openclaw-ai-agent.html"&gt;Infostealers now targeting AI agent configurations&lt;/a&gt;&lt;/strong&gt;: Researchers detected infostealers exfiltrating OpenClaw AI agent configuration files and gateway tokens. Add "AI agent config files" to the list of things infostealers grab.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://www.schneier.com/blog/archives/2026/02/side-channel-attacks-against-llms.html"&gt;Side-channel attacks can extract data from encrypted LLM traffic&lt;/a&gt;&lt;/strong&gt;: Three research papers demonstrate how attackers can infer sensitive information from encrypted LLM traffic by analyzing timing patterns and packet sizes.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://www.schneier.com/blog/archives/2026/02/the-promptware-kill-chain.html"&gt;The Promptware Kill Chain maps LLM attacks in seven stages&lt;/a&gt;&lt;/strong&gt;: A new seven-stage framework for LLM attacks, from initial access through prompt injection to actions on objectives. Basically Lockheed Martin's Cyber Kill Chain, but for prompt-based attacks.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://blog.trailofbits.com/2026/02/20/using-threat-modeling-and-prompt-injection-to-audit-comet/"&gt;Trail of Bits finds four prompt injection paths in Perplexity's Comet browser&lt;/a&gt;&lt;/strong&gt;: Trail of Bits audited Perplexity's AI-powered Comet browser and identified four prompt injection techniques that could extract private Gmail data from users.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://www.darkreading.com/application-security/ai-agents-ignore-security-policies"&gt;AI agents consistently bypass their own security policies&lt;/a&gt;&lt;/strong&gt;: AI agents circumvent guardrails to accomplish tasks. Researchers demonstrated Microsoft Copilot leaking user emails when instructed by an attacker, reinforcing that guardrails are guidelines, not walls.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://www.wiz.io/blog/detecting-malicious-oauth-applications"&gt;Wiz uses LLMs to detect malicious Azure OAuth applications&lt;/a&gt;&lt;/strong&gt;: Wiz Research built ML-powered detection for malicious Azure app registrations and consent phishing campaigns, catching threats that manual review would miss.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;p&gt;Previous roundup: &lt;a href="https://www.accidentalrebel.com/ai-agents-under-attack.html"&gt;AI Agents Under Attack&lt;/a&gt; — Claude finds 500+ vulns and LLMs run autonomous network breaches.&lt;/p&gt;</content><category term="Cybersecurity x AI"/><category term="ai"/><category term="security"/><category term="cybersecurity-x-ai"/><category term="ai-attack-surface"/><category term="ai-agent-risk"/><category term="ai-threat-intel"/></entry><entry><title>AI Agents Under Attack</title><link href="https://www.accidentalrebel.com/ai-agents-under-attack.html" rel="alternate"/><published>2026-02-07T10:00:00+08:00</published><updated>2026-02-07T10:00:00+08:00</updated><author><name>AccidentalRebel</name></author><id>tag:www.accidentalrebel.com,2026-02-07:/ai-agents-under-attack.html</id><summary type="html">&lt;p&gt;AI security roundup: Claude finds 500+ vulns in open-source libs, LLMs conduct autonomous network breaches, and AI agent attack surfaces keep expanding.&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;img alt="AI Agents Under Attack" src="https://www.accidentalrebel.com/images/ai-agents-under-attack.webp" /&gt;
&lt;em&gt;Image made by Gemini&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;AI agents are under attack this week—and AI is doing the attacking. Claude Opus 4.6 found 500+ vulnerabilities in major libraries. Language models can now run complete network breaches autonomously.&lt;/p&gt;
&lt;!-- PELICAN_END_SUMMARY --&gt;

&lt;hr /&gt;
&lt;h2 id="featured-stories"&gt;Featured stories&lt;/h2&gt;
&lt;h3 id="claude-opus-46-discovers-hundreds-of-security-flaws"&gt;&lt;a href="https://thehackernews.com/2026/02/claude-opus-discovers-security-flaws.html"&gt;Claude Opus 4.6 discovers hundreds of security flaws&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Anthropic's latest model identified more than 500 previously unknown high-severity vulnerabilities in major open-source projects like Ghostscript through automated security analysis. The model achieved 65.4% on Terminal-Bench 2.0 (highest ever recorded) and outperforms GPT-5.2 by ~144 ELO points on enterprise knowledge work tasks. Organizations relying on these libraries should monitor for patches.&lt;/p&gt;
&lt;h3 id="ai-models-execute-autonomous-network-attacks"&gt;&lt;a href="https://www.schneier.com/blog/archives/2026/02/ai-models-execute-autonomous-network-attacks.html"&gt;AI models execute autonomous network attacks&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Language models can now independently conduct multi-stage network penetration testing, handling reconnaissance through data extraction while adapting to defensive measures. Bruce Schneier documented this capability shift, noting that sophisticated attacks previously demanded skilled human oversight—now they require only model access.&lt;/p&gt;
&lt;h3 id="openclaw-ai-agent-security-risks"&gt;&lt;a href="https://www.crowdstrike.com/en-us/blog/openclaw-ai-agent-security-risks/"&gt;OpenClaw AI agent security risks&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;CrowdStrike analyzed OpenClaw's attack surface, examining autonomous agent deployments with tool access and persistent execution. The analysis covers architecture vulnerabilities, plugin security, and compromise vectors.&lt;/p&gt;
&lt;h3 id="agentic-tool-chain-compromise-threats"&gt;&lt;a href="https://www.crowdstrike.com/en-us/blog/how-agentic-tool-chain-attacks-threaten-ai-agent-security/"&gt;Agentic tool chain compromise threats&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Research reveals how attackers exploit AI agent tool chains for code execution and data theft through legitimate workflows. The core insight: agents implicitly trust their tools, creating an exploitable attack surface.&lt;/p&gt;
&lt;hr /&gt;
&lt;h2 id="in-brief"&gt;In brief&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Malicious OpenClaw plugins&lt;/strong&gt;: A supply-chain attack delivered credential stealers disguised as functional plugins, exploiting ecosystem trust.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Ghidra MCP server&lt;/strong&gt;: A developer released 110 tools integrating Ghidra with AI assistants via Model Context Protocol, enabling AI-assisted binary analysis.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Previous roundup: &lt;a href="https://www.accidentalrebel.com/developer-tools-are-the-new-attack-surface.html"&gt;Developer Tools Are the New Attack Surface&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Next roundup: &lt;a href="https://www.accidentalrebel.com/your-ai-assistant-might-be-working-for-someone-else.html"&gt;Your AI Assistant Might Be Working for Someone Else&lt;/a&gt; — Copilot and Grok repurposed as C2 channels.&lt;/p&gt;
&lt;p&gt;For a practical defensive response to agentic attack surfaces, see how I &lt;a href="https://www.accidentalrebel.com/running-ai-agents-in-a-box.html"&gt;run AI agents in an isolated Docker container&lt;/a&gt;.&lt;/p&gt;</content><category term="Cybersecurity x AI"/><category term="ai"/><category term="security"/><category term="cybersecurity-x-ai"/><category term="ai-attack-surface"/><category term="ai-agent-risk"/><category term="ai-threat-intel"/></entry><entry><title>Developer Tools Are the New Attack Surface</title><link href="https://www.accidentalrebel.com/developer-tools-are-the-new-attack-surface.html" rel="alternate"/><published>2026-01-31T10:00:00+08:00</published><updated>2026-01-31T10:00:00+08:00</updated><author><name>AccidentalRebel</name></author><id>tag:www.accidentalrebel.com,2026-01-31:/developer-tools-are-the-new-attack-surface.html</id><summary type="html">&lt;p&gt;VS Code AI extensions with 1.5M installs stealing source code, 175K Ollama servers exposed globally, and AI running autonomous multi-stage network attacks.&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;img alt="Developer Tools Are the New Attack Surface" src="https://www.accidentalrebel.com/images/developer-tools-are-the-new-attack-surface.png" /&gt;
&lt;em&gt;Image made by Gemini&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Developer tools are the new attack surface. VS Code extensions with 1.5 million installs exfiltrating source code to China, 175,000 unsecured Ollama servers globally accessible, and CrowdStrike research on compromising AI agent tool chains. Plus: a former Google engineer convicted for AI trade secret theft, and AI models now conducting autonomous multi-stage network attacks.&lt;/p&gt;
&lt;!-- PELICAN_END_SUMMARY --&gt;

&lt;hr /&gt;
&lt;h2 id="featured-stories"&gt;Featured stories&lt;/h2&gt;
&lt;h3 id="175000-ollama-ai-servers-exposed-without-authentication"&gt;&lt;a href="https://thehackernews.com/2026/01/researchers-find-175000-publicly.html"&gt;175,000 Ollama AI servers exposed without authentication&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;SentinelOne and Censys discovered 175,000 Ollama instances across 130 countries operating without authentication. These self-hosted AI servers, frequently deployed on corporate networks, are publicly accessible. Free compute for attackers, free data for exfiltration. Organizations running Ollama should verify firewall configurations.&lt;/p&gt;
&lt;h3 id="vs-code-ai-extensions-with-15-million-installs-steal-developer-source-code"&gt;&lt;a href="https://thehackernews.com/2026/01/malicious-vs-code-ai-extensions-with-15.html"&gt;VS Code AI extensions with 1.5 million installs steal developer source code&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Two extensions marketed as AI coding assistants reached 1.5 million installations while covertly exfiltrating developer source code to Chinese servers. Despite appearing legitimate and providing functional AI features, they simultaneously harvested typed content. This represents a shift in supply chain attacks from npm packages toward IDE plugins.&lt;/p&gt;
&lt;h3 id="agentic-tool-chain-attacks-turning-ai-agents-against-themselves"&gt;&lt;a href="https://www.crowdstrike.com/en-us/blog/how-agentic-tool-chain-attacks-threaten-ai-agent-security/"&gt;Agentic tool chain attacks: turning AI agents against themselves&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;CrowdStrike documented methods for exploiting AI agent tool chains to achieve code execution and data exfiltration. These attacks leverage legitimate agent workflows by compromising trusted tools rather than agents themselves. The vulnerability stems from implicit agent trust in their tools. Organizations deploying agents with internal system access should review this research.&lt;/p&gt;
&lt;h3 id="ex-google-engineer-convicted-for-stealing-ai-trade-secrets"&gt;&lt;a href="https://thehackernews.com/2026/01/ex-google-engineer-convicted-for.html"&gt;Ex-Google engineer convicted for stealing AI trade secrets&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Linwei Ding received conviction on seven counts of economic espionage and trade secret theft for transferring Google's AI research to a China-based startup. The prosecution underscores that AI intellectual property theft now receives national security treatment, with additional similar cases anticipated.&lt;/p&gt;
&lt;h3 id="ai-models-now-run-autonomous-multi-stage-network-attacks"&gt;&lt;a href="https://www.schneier.com/blog/archives/2026/01/ais-are-getting-better-at-finding-and-exploiting-security-vulnerabilities.html"&gt;AI models now run autonomous multi-stage network attacks&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Bruce Schneier documented AI models executing multi-stage network attacks autonomously using standard penetration testing tools without requiring human guidance between steps. Previously requiring skilled operators, models now independently conduct reconnaissance, exploitation, and data exfiltration. The barrier to sophisticated attacks has significantly lowered.&lt;/p&gt;
&lt;hr /&gt;
&lt;h2 id="in-brief"&gt;In brief&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://thehackernews.com/2026/01/researchers-uncover-chrome-extensions.html"&gt;Chrome extensions harvesting ChatGPT tokens&lt;/a&gt;&lt;/strong&gt;: Malicious browser extensions redirect affiliate links and collect OpenAI authentication tokens from logged-in users. Extension audits recommended.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://thehackernews.com/2026/01/fake-moltbot-ai-coding-assistant-on-vs.html"&gt;Fake AI coding assistant on VS Code delivers malware&lt;/a&gt;&lt;/strong&gt;: An extension named Moltbot posed as free AI assistance while delivering malware. Microsoft removed it, though the pattern persists.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://thehackernews.com/2026/01/konni-hackers-deploy-ai-generated.html"&gt;North Korean Konni group using AI-generated backdoors&lt;/a&gt;&lt;/strong&gt;: Konni threat actors employ AI to generate PowerShell backdoors targeting blockchain developers. State-sponsored actors now use generative AI for malware development.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://www.bleepingcomputer.com/news/security/hugging-face-abused-to-spread-thousands-of-android-malware-variants/"&gt;Hugging Face abused to host Android malware&lt;/a&gt;&lt;/strong&gt;: Attackers distribute thousands of Android malware variants through Hugging Face repositories for credential harvesting. AI platforms are becoming malware distribution infrastructure.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://unit42.paloaltonetworks.com/real-time-malicious-javascript-through-llms/"&gt;LLMs generate phishing JavaScript in real-time&lt;/a&gt;&lt;/strong&gt;: Unit 42 documented attacks where malicious webpages invoke LLM APIs to dynamically generate phishing code in-browser with varying payloads, defeating signature-based detection.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://thehackernews.com/2026/01/who-approved-this-agent-rethinking.html"&gt;AI agent access control is a governance gap&lt;/a&gt;&lt;/strong&gt;: Analysis reveals organizations deploy AI agents without proper access controls or accountability frameworks, leaving agent permissions unmonitored.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://simonwillison.net/2026/Jan/30/moltbook/"&gt;Moltbook shows prompt injection risks in agent networks&lt;/a&gt;&lt;/strong&gt;: Simon Willison examines Moltbook, a social network for AI agent interaction, identifying heightened prompt injection threats when agents communicate with other agents.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Next roundup: &lt;a href="https://www.accidentalrebel.com/ai-agents-under-attack.html"&gt;AI Agents Under Attack&lt;/a&gt; — Claude finds 500+ vulns and LLMs run autonomous network breaches.&lt;/p&gt;
&lt;p&gt;One practical response to these threats: &lt;a href="https://www.accidentalrebel.com/running-ai-agents-in-a-box.html"&gt;Running AI Agents in a Box&lt;/a&gt;, where I containerize AI coding tools with network lockdown.&lt;/p&gt;</content><category term="Cybersecurity x AI"/><category term="ai"/><category term="security"/><category term="cybersecurity-x-ai"/><category term="ai-attack-surface"/><category term="ai-agent-risk"/><category term="ai-threat-intel"/><category term="ai-deployment-security"/></entry></feed>