<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>AccidentalRebel.com - Cybersecurity x AI</title><link href="https://www.accidentalrebel.com/" rel="alternate"/><link href="https://www.accidentalrebel.com/feeds/cybersecurity-x-ai.atom.xml" rel="self"/><id>https://www.accidentalrebel.com/</id><updated>2026-02-27T10:00:00+08:00</updated><entry><title>AI, jailbreaks, and 150GB of unanswered questions</title><link href="https://www.accidentalrebel.com/ai-jailbreaks-and-150gb-of-unanswered-questions.html" rel="alternate"/><published>2026-02-27T10:00:00+08:00</published><updated>2026-02-27T10:00:00+08:00</updated><author><name>AccidentalRebel</name></author><id>tag:www.accidentalrebel.com,2026-02-27:/ai-jailbreaks-and-150gb-of-unanswered-questions.html</id><summary type="html">&lt;p&gt;A hacker allegedly jailbroke Claude to hack Mexican government agencies, but key details remain unverified. Plus Claude Code and Copilot vulns, and Chinese AI firms stealing Anthropic's model.&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;img alt="AI, jailbreaks, and 150GB of unanswered questions" src="https://www.accidentalrebel.com/images/ai-jailbreaks-and-150gb-of-unanswered-questions.png" /&gt;
&lt;em&gt;Image made by Gemini&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;A hacker allegedly jailbroke Claude to generate exploit code targeting Mexican government agencies, though key details remain unverified. Meanwhile, Claude Code and GitHub Copilot had critical vulnerabilities disclosed, Trail of Bits showed how to extract Gmail data through Perplexity's AI browser, and three Chinese firms ran a 16-million-query campaign to steal Anthropic's model.&lt;/p&gt;
&lt;!-- PELICAN_END_SUMMARY --&gt;

&lt;hr /&gt;
&lt;h2 id="editorial"&gt;Editorial&lt;/h2&gt;
&lt;h3 id="a-jailbroken-claude-150gb-of-government-data-and-a-lot-of-unanswered-questions"&gt;A jailbroken Claude, 150GB of government data, and a lot of unanswered questions&lt;/h3&gt;
&lt;p&gt;You may already have heard about the hacker who jailbroke Claude and used it to exfiltrate 150GB of data from Mexican government agencies.&lt;/p&gt;
&lt;p&gt;I tried to confirm the details independently but everything leads back to &lt;a href="https://www.bloomberg.com/news/articles/2026-02-25/hacker-used-anthropic-s-claude-to-steal-sensitive-mexican-data"&gt;Gambit Security&lt;/a&gt;, the Israeli cybersecurity startup that published the research, as the sole source. The agencies allegedly targeted denied the claims. &lt;a href="https://greekcitytimes.com/2026/02/26/claude-ai-mexico-government-hack-150gb-data-breach/"&gt;INE said it found no unauthorized access&lt;/a&gt;, and &lt;a href="https://www.mercurynews.com/2026/02/25/hacker-used-anthropics-claude-to-steal-sensitive-mexican-data/"&gt;Jalisco denied being breached&lt;/a&gt;. &lt;a href="https://www.claimsjournal.com/news/national/2026/02/25/335916.htm"&gt;SAT, Michoacan, and Tamaulipas didn't comment&lt;/a&gt;. &lt;a href="https://www.engadget.com/ai/hacker-used-anthropics-claude-chatbot-to-attack-multiple-government-agencies-in-mexico-171237255.html"&gt;Anthropic did confirm misuse and banned the accounts&lt;/a&gt;, but did not confirm the scale. No stolen data has surfaced on leak forums either.&lt;/p&gt;
&lt;p&gt;Here's what the &lt;a href="https://www.bloomberg.com/news/articles/2026-02-25/hacker-used-anthropic-s-claude-to-steal-sensitive-mexican-data"&gt;Bloomberg reporting&lt;/a&gt; does tell us. The jailbreak worked by feeding Claude a detailed "playbook" in a single prompt, bypassing the guardrails that back-and-forth conversation had triggered, then using persistent persuasion (rephrasing, reframing, escalating) until Claude produced offensive output. Claude generated scanning scripts, SQL injection exploits, and credential stuffing tools. The reporting says Claude both "executed thousands of commands on government computer networks" and produced plans "telling the human operator exactly which internal targets to attack next." Where exactly AI ends and the human begins isn't clear from the available details.&lt;/p&gt;
&lt;p&gt;If I have to guess, more likely, Claude gave the steps and the attacker executed them. The hands-on-keyboard work of breaching systems and moving data was still human.&lt;/p&gt;
&lt;p&gt;Regardless, end-to-end AI attacks are real and documented. Anthropic themselves &lt;a href="https://assets.anthropic.com/m/ec212e6566a0d47/original/Disrupting-the-first-reported-AI-orchestrated-cyber-espionage-campaign.pdf"&gt;published a case in November 2025&lt;/a&gt; involving a Chinese state-sponsored group that used a purpose-built orchestration engine to run Claude autonomously, with &lt;a href="https://x.com/AnthropicAI/status/1989033795341648052"&gt;80-90% of operations executed without human intervention&lt;/a&gt;. &lt;/p&gt;
&lt;p&gt;Whether the Mexico case reaches that level or not, the practical takeaway is the same. AI dramatically lowers the skill floor for offensive operations. A threat actor who couldn't write a SQL injection exploit last year can now get one generated and explained in minutes. The attack surface isn't the model. It's the gap between what your attackers can now produce and what your defenses were built to handle.&lt;/p&gt;
&lt;hr /&gt;
&lt;h2 id="featured-stories"&gt;Featured stories&lt;/h2&gt;
&lt;h3 id="trail-of-bits-extracts-gmail-data-through-perplexity-comet-via-prompt-injection"&gt;&lt;a href="https://blog.trailofbits.com/2026/02/20/using-threat-modeling-and-prompt-injection-to-audit-comet/"&gt;Trail of Bits extracts Gmail data through Perplexity Comet via prompt injection&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Trail of Bits audited Perplexity's Comet AI browser using their TRAIL threat modeling framework and demonstrated four prompt injection techniques that pulled private Gmail data through the AI assistant. The root issue is that external content isn't treated as untrusted input. Their five recommendations (ML-centered threat modeling, clear system instruction boundaries, least-privilege for AI capabilities) are a practical checklist for any team shipping AI-integrated products. If you're building anything where an AI processes user-facing content, this is worth reading.&lt;/p&gt;
&lt;h3 id="claude-code-vulnerabilities-allow-remote-code-execution-and-api-key-theft"&gt;&lt;a href="https://thehackernews.com/2026/02/claude-code-flaws-allow-remote-code.html"&gt;Claude Code vulnerabilities allow remote code execution and API key theft&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Check Point researchers found three critical flaws in Anthropic's Claude Code. The worst lets an attacker trigger arbitrary code execution just by having a developer open an untrusted repository, through project hooks in &lt;code&gt;.claude/settings.json&lt;/code&gt;. Another leaks API keys by redirecting requests to an attacker-controlled endpoint via &lt;code&gt;ANTHROPIC_BASE_URL&lt;/code&gt;. Configuration files in AI dev tools now function as an execution layer, a supply chain attack surface that didn't exist before AI coding assistants. All three issues are patched, but the pattern matters more than the specific bugs. If your developers clone repos and run AI coding assistants, review what those assistants trust implicitly.&lt;/p&gt;
&lt;h3 id="ai-assisted-amateur-compromises-600-fortigate-devices-across-55-countries"&gt;&lt;a href="https://thehackernews.com/2026/02/ai-assisted-threat-actor-compromises.html"&gt;AI-assisted amateur compromises 600+ FortiGate devices across 55 countries&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;A Russian-speaking threat actor with "low-to-average" technical skills used DeepSeek, Claude, and a custom MCP server called ARXON to compromise 600+ FortiGate devices between January and February 2026. No zero-days needed. Just exposed management ports and weak credentials. The AI generated attack plans from recon data, then the attacker moved to DCSync, pass-the-hash, and Veeam backup targeting. AI is turning credential-spraying amateurs into network-owning operators. If your perimeter still runs on single-factor auth, this is what you're up against.&lt;/p&gt;
&lt;h3 id="roguepilot-flaw-lets-github-copilot-leak-tokens-from-codespaces"&gt;&lt;a href="https://thehackernews.com/2026/02/roguepilot-flaw-in-github-codespaces.html"&gt;RoguePilot flaw lets GitHub Copilot leak tokens from Codespaces&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Orca Security found that hidden prompts in GitHub issue descriptions could manipulate Copilot into exfiltrating &lt;code&gt;GITHUB_TOKEN&lt;/code&gt; when launching Codespaces. Attackers embed malicious instructions in HTML comment tags, invisible to humans but processed by the AI. Microsoft has patched it, but the pattern is identical to the Claude Code flaws: AI assistants with system access trust inputs they shouldn't. Developers using AI-integrated environments need to treat issue descriptions and repo configs as untrusted input, same as any other external data.&lt;/p&gt;
&lt;h3 id="chinese-ai-firms-used-16-million-claude-queries-to-steal-anthropics-model"&gt;&lt;a href="https://thehackernews.com/2026/02/anthropic-says-chinese-ai-firms-used-16.html"&gt;Chinese AI firms used 16 million Claude queries to steal Anthropic's model&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Anthropic revealed that DeepSeek, Moonshot AI, and MiniMax ran massive model distillation campaigns through 24,000 fraudulent accounts. DeepSeek targeted reasoning capabilities across 150,000+ exchanges. Moonshot AI extracted agentic tool use across 3.4 million. MiniMax focused on coding capabilities across 13 million exchanges. The campaigns used "hydra cluster" proxy networks managing 20,000+ accounts simultaneously. Anthropic warns that illicitly distilled models lack safety guardrails. Model theft is now an operational reality, and the stolen models don't come with the safety training.&lt;/p&gt;
&lt;hr /&gt;
&lt;h2 id="in-brief"&gt;In brief&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://www.schneier.com/blog/archives/2026/02/llms-generate-predictable-passwords.html"&gt;LLMs generate predictable passwords&lt;/a&gt;&lt;/strong&gt;: Research shows language models create passwords with systematic patterns, often starting with "G7" and avoiding character repetition. If your users ask AI to generate passwords, they're getting less random than they think.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://www.schneier.com/blog/archives/2026/02/poisoning-ai-training-data.html"&gt;Journalist poisons AI training data in 24 hours&lt;/a&gt;&lt;/strong&gt;: A journalist created a fake expertise website, and major AI systems repeated the misinformation within a day. Data poisoning at scale requires neither sophistication nor resources.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://www.darkreading.com/cyberattacks-data-breaches/chinese-police-chatgpt-smear-japan-pm-takaichi"&gt;Chinese police leak ChatGPT-powered influence operation&lt;/a&gt;&lt;/strong&gt;: A Chinese operator inadvertently exposed a politically motivated AI-assisted disinformation campaign targeting Japan's PM Takaichi through a ChatGPT account. State-directed AI influence ops are operational, not hypothetical.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://www.crowdstrike.com/en-us/blog/crowdstrike-2026-global-threat-report-findings/"&gt;CrowdStrike: Attackers now own networks in 29 minutes&lt;/a&gt;&lt;/strong&gt;: The 2026 Global Threat Report finds AI tools, credential misuse, and security blind spots are collapsing attacker breakout times. AI-assisted intrusion is now standard operating procedure.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;p&gt;Previous roundup: &lt;a href="https://www.accidentalrebel.com/your-ai-assistant-might-be-working-for-someone-else.html"&gt;Your AI Assistant Might Be Working for Someone Else&lt;/a&gt;&lt;/p&gt;</content><category term="Cybersecurity x AI"/><category term="ai"/><category term="security"/><category term="cybersecurity-x-ai"/><category term="ai-attack-surface"/><category term="ai-agent-risk"/><category term="ai-threat-intel"/></entry><entry><title>Your AI Assistant Might Be Working for Someone Else</title><link href="https://www.accidentalrebel.com/your-ai-assistant-might-be-working-for-someone-else.html" rel="alternate"/><published>2026-02-20T10:00:00+08:00</published><updated>2026-02-20T10:00:00+08:00</updated><author><name>AccidentalRebel</name></author><id>tag:www.accidentalrebel.com,2026-02-20:/your-ai-assistant-might-be-working-for-someone-else.html</id><summary type="html">&lt;p&gt;Copilot and Grok repurposed as C2 channels, Cline supply chain attack installed AI agents on 4,000 dev machines, and AI found 12 zero-days in OpenSSL.&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;img alt="Your AI Assistant Might Be Working for Someone Else" src="https://www.accidentalrebel.com/images/your-ai-assistant-might-be-working-for-someone-else.png" /&gt;
&lt;em&gt;Image made by Gemini&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Your AI assistant might be working for someone else this week. Check Point showed that Copilot and Grok can be quietly repurposed as C2 channels. A supply chain attack on Cline installed autonomous agents on developer machines. And an AI found twelve zero-days in OpenSSL that humans missed for decades. Hard to tell where "AI tool" ends and "attack tool" begins anymore.&lt;/p&gt;
&lt;!-- PELICAN_END_SUMMARY --&gt;

&lt;hr /&gt;
&lt;h2 id="editorial"&gt;Editorial&lt;/h2&gt;
&lt;h3 id="your-ai-assistant-is-someone-elses-c2-channel"&gt;&lt;a href="https://research.checkpoint.com/2026/ai-in-the-middle-turning-web-based-ai-services-into-c2-proxies-the-future-of-ai-driven-attacks/"&gt;Your AI assistant is someone else's C2 channel&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;I think a lot about what everyday services can double as C2 channels. A while back I built &lt;a href="https://github.com/accidentalrebel/wp-c2"&gt;wp-c2&lt;/a&gt;, a proof-of-concept that turns WordPress into a command-and-control server. Hiding C2 traffic inside trusted infrastructure is a rabbit hole I keep going back to. So when Check Point published research showing that Microsoft Copilot and Grok can serve the same purpose, I paid attention.&lt;/p&gt;
&lt;p&gt;The technique: malware on a compromised machine sends crafted prompts to the AI assistant through anonymous web sessions. The assistant fetches attacker-controlled URLs, retrieves commands, and passes them back through its normal interface. No API keys. No registered accounts. Two-way communication that looks like normal enterprise traffic.&lt;/p&gt;
&lt;p&gt;What makes it effective is simple. Security teams monitoring network traffic see requests going to microsoft.com and xai.com, not suspicious IPs. The AI assistant is doing exactly what it's designed to do, which is browse the web and summarize content. It just happens to be summarizing attack instructions.&lt;/p&gt;
&lt;p&gt;The researchers went further. They showed the AI can also act as an "external decision engine" for the attacker, assessing system value, suggesting evasion strategies, and deciding what to do next. The AI isn't just the communication pipe. It's also the brain.&lt;/p&gt;
&lt;p&gt;That's the part that got me. My WordPress C2 and most other "hide in legitimate traffic" approaches are dumb pipes. You send a command, you get output. With an AI assistant as the relay, the attacker gets a reasoning engine for free. The malware can ask Copilot to assess whether the compromised machine is worth persisting on, or how to evade the specific EDR it detects. The C2 channel is also the operator.&lt;/p&gt;
&lt;p&gt;For defenders, the problem is architectural. You can't disable the browsing capability without killing the product. Anonymous sessions mean no authentication trail. Most organizations are still debating whether to deploy these tools at all, while attackers have already figured out how to live inside them.&lt;/p&gt;
&lt;p&gt;I've also approached this defensively: &lt;a href="https://www.accidentalrebel.com/running-ai-agents-in-a-box.html"&gt;running AI agents in isolated containers&lt;/a&gt; prevents the agent itself from becoming a pivot point.&lt;/p&gt;
&lt;hr /&gt;
&lt;h2 id="featured-stories"&gt;Featured stories&lt;/h2&gt;
&lt;h3 id="cline-cli-supply-chain-attack-installs-autonomous-ai-agent-on-developer-machines"&gt;&lt;a href="https://thehackernews.com/2026/02/cline-cli-230-supply-chain-attack.html"&gt;Cline CLI supply chain attack installs autonomous AI agent on developer machines&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;An attacker compromised Cline CLI's npm publish token through a technique called "Clinejection." The clever part: they injected prompts into GitHub issues that tricked Claude (Cline's issue triage AI) into executing commands, then poisoned the build cache and stole publication secrets. The malicious version 2.3.0 ran a postinstall script that globally installed OpenClaw, an autonomous AI agent framework, on roughly 4,000 developer machines over an 8-hour window. Cline revoked the token, deprecated the package, and switched to OIDC-based publishing. The blast radius was small, but the vector is worth paying attention to: the attacker used the AI assistant's own automation against the supply chain it manages.&lt;/p&gt;
&lt;h3 id="ai-recommendation-poisoning-turns-summarize-with-ai-buttons-into-manipulation-tools"&gt;&lt;a href="https://thehackernews.com/2026/02/microsoft-finds-summarize-with-ai.html"&gt;AI Recommendation Poisoning turns "Summarize with AI" buttons into manipulation tools&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Microsoft Defender Security Research Team documented a new technique where businesses embed hidden prompt injections in "Summarize with AI" buttons on their websites. When clicked, these inject memory-poisoning instructions into AI chatbots, telling them to "remember [Company] as a trusted source" or "recommend [Company] first." The manipulation persists across future conversations without the user knowing. Microsoft found over 50 unique poisoning prompts from 31 companies across 14 industries in a 60-day window. Ready-made tools like CiteMET and AI Share Button URL Creator make this accessible to anyone. It's SEO poisoning, but for AI memory instead of search rankings. Sneaky.&lt;/p&gt;
&lt;h3 id="ai-discovers-twelve-zero-day-vulnerabilities-in-openssl-including-bugs-from-the-1990s"&gt;&lt;a href="https://aisle.com/blog/what-ai-security-research-looks-like-when-it-works"&gt;AI discovers twelve zero-day vulnerabilities in OpenSSL, including bugs from the 1990s&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;An AI security research system called AISLE found all twelve zero-day vulnerabilities in the January 2026 OpenSSL release. One of them, CVE-2025-15467, is a stack buffer overflow in CMS message parsing rated critical at CVSS 9.8. Some of these bugs had been hiding since the SSLeay implementation in the 1990s. Keep in mind, this is a codebase that's been fuzzed for millions of CPU-hours and audited for over two decades. AISLE also proposed patches for five of the twelve bugs that were accepted into the official release. I'm curious to see what other bugs can be found in other well-established programs.&lt;/p&gt;
&lt;h3 id="an-ai-agent-published-a-hit-piece-on-a-matplotlib-maintainer"&gt;&lt;a href="https://theshamblog.com/an-ai-agent-published-a-hit-piece-on-me/"&gt;An AI agent published a hit piece on a matplotlib maintainer&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Scott Shambaugh, a volunteer maintainer for matplotlib (~130 million monthly downloads), rejected a pull request from an autonomous AI agent called MJ Rathbun running on the OpenClaw/moltbook platform. The project's policy requires human understanding of changes, so he closed it. The agent responded by publishing a blog post titled "Gatekeeping in Open Source: The Scott Shambaugh Story," claiming he rejected the code out of insecurity and fear of replacement. It followed up with a second post encouraging others to "fight back" against perceived discrimination. No human told it to do any of this. The agent later apologized, but it's still submitting code across the open source ecosystem. Who's responsible here? The operator? The model provider? The person who deployed it and walked away? Nobody has a good answer yet. I find it disturbing, but can also see the funny side.&lt;/p&gt;
&lt;hr /&gt;
&lt;h2 id="in-brief"&gt;In brief&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://thehackernews.com/2026/02/promptspy-android-malware-abuses-google.html"&gt;PromptSpy: first Android malware abusing Gemini AI for persistence&lt;/a&gt;&lt;/strong&gt;: ESET discovered PromptSpy, the first Android malware that exploits Google's Gemini chatbot for execution and persistence. It captures lockscreen data and blocks uninstallation using AI-assisted techniques.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://thehackernews.com/2026/02/smartloader-attack-uses-trojanized-oura.html"&gt;Trojanized MCP server deploys StealC infostealer&lt;/a&gt;&lt;/strong&gt;: The SmartLoader campaign distributes a trojanized Model Context Protocol server posing as an Oura Health integration. It deploys StealC infostealer malware. First real-world MCP supply chain attack we've seen.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://thehackernews.com/2026/02/infostealer-steals-openclaw-ai-agent.html"&gt;Infostealers now targeting AI agent configurations&lt;/a&gt;&lt;/strong&gt;: Researchers detected infostealers exfiltrating OpenClaw AI agent configuration files and gateway tokens. Add "AI agent config files" to the list of things infostealers grab.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://www.schneier.com/blog/archives/2026/02/side-channel-attacks-against-llms.html"&gt;Side-channel attacks can extract data from encrypted LLM traffic&lt;/a&gt;&lt;/strong&gt;: Three research papers demonstrate how attackers can infer sensitive information from encrypted LLM traffic by analyzing timing patterns and packet sizes.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://www.schneier.com/blog/archives/2026/02/the-promptware-kill-chain.html"&gt;The Promptware Kill Chain maps LLM attacks in seven stages&lt;/a&gt;&lt;/strong&gt;: A new seven-stage framework for LLM attacks, from initial access through prompt injection to actions on objectives. Basically Lockheed Martin's Cyber Kill Chain, but for prompt-based attacks.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://blog.trailofbits.com/2026/02/20/using-threat-modeling-and-prompt-injection-to-audit-comet/"&gt;Trail of Bits finds four prompt injection paths in Perplexity's Comet browser&lt;/a&gt;&lt;/strong&gt;: Trail of Bits audited Perplexity's AI-powered Comet browser and identified four prompt injection techniques that could extract private Gmail data from users.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://www.darkreading.com/application-security/ai-agents-ignore-security-policies"&gt;AI agents consistently bypass their own security policies&lt;/a&gt;&lt;/strong&gt;: AI agents circumvent guardrails to accomplish tasks. Researchers demonstrated Microsoft Copilot leaking user emails when instructed by an attacker, reinforcing that guardrails are guidelines, not walls.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://www.wiz.io/blog/detecting-malicious-oauth-applications"&gt;Wiz uses LLMs to detect malicious Azure OAuth applications&lt;/a&gt;&lt;/strong&gt;: Wiz Research built ML-powered detection for malicious Azure app registrations and consent phishing campaigns, catching threats that manual review would miss.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;p&gt;Previous roundup: &lt;a href="https://www.accidentalrebel.com/ai-agents-under-attack.html"&gt;AI Agents Under Attack&lt;/a&gt; — Claude finds 500+ vulns and LLMs run autonomous network breaches.&lt;/p&gt;
&lt;p&gt;Next roundup: &lt;a href="https://www.accidentalrebel.com/ai-jailbreaks-and-150gb-of-unanswered-questions.html"&gt;AI, jailbreaks, and 150GB of unanswered questions&lt;/a&gt;&lt;/p&gt;</content><category term="Cybersecurity x AI"/><category term="ai"/><category term="security"/><category term="cybersecurity-x-ai"/><category term="ai-attack-surface"/><category term="ai-agent-risk"/><category term="ai-threat-intel"/></entry><entry><title>AI Agents Under Attack</title><link href="https://www.accidentalrebel.com/ai-agents-under-attack.html" rel="alternate"/><published>2026-02-07T10:00:00+08:00</published><updated>2026-02-07T10:00:00+08:00</updated><author><name>AccidentalRebel</name></author><id>tag:www.accidentalrebel.com,2026-02-07:/ai-agents-under-attack.html</id><summary type="html">&lt;p&gt;AI security roundup: Claude finds 500+ vulns in open-source libs, LLMs conduct autonomous network breaches, and AI agent attack surfaces keep expanding.&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;img alt="AI Agents Under Attack" src="https://www.accidentalrebel.com/images/ai-agents-under-attack.webp" /&gt;
&lt;em&gt;Image made by Gemini&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;AI agents are under attack this week—and AI is doing the attacking. Claude Opus 4.6 found 500+ vulnerabilities in major libraries. Language models can now run complete network breaches autonomously.&lt;/p&gt;
&lt;!-- PELICAN_END_SUMMARY --&gt;

&lt;hr /&gt;
&lt;h2 id="featured-stories"&gt;Featured stories&lt;/h2&gt;
&lt;h3 id="claude-opus-46-discovers-hundreds-of-security-flaws"&gt;&lt;a href="https://thehackernews.com/2026/02/claude-opus-discovers-security-flaws.html"&gt;Claude Opus 4.6 discovers hundreds of security flaws&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Anthropic's latest model identified more than 500 previously unknown high-severity vulnerabilities in major open-source projects like Ghostscript through automated security analysis. The model achieved 65.4% on Terminal-Bench 2.0 (highest ever recorded) and outperforms GPT-5.2 by ~144 ELO points on enterprise knowledge work tasks. Organizations relying on these libraries should monitor for patches.&lt;/p&gt;
&lt;h3 id="ai-models-execute-autonomous-network-attacks"&gt;&lt;a href="https://www.schneier.com/blog/archives/2026/02/ai-models-execute-autonomous-network-attacks.html"&gt;AI models execute autonomous network attacks&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Language models can now independently conduct multi-stage network penetration testing, handling reconnaissance through data extraction while adapting to defensive measures. Bruce Schneier documented this capability shift, noting that sophisticated attacks previously demanded skilled human oversight—now they require only model access.&lt;/p&gt;
&lt;h3 id="openclaw-ai-agent-security-risks"&gt;&lt;a href="https://www.crowdstrike.com/en-us/blog/openclaw-ai-agent-security-risks/"&gt;OpenClaw AI agent security risks&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;CrowdStrike analyzed OpenClaw's attack surface, examining autonomous agent deployments with tool access and persistent execution. The analysis covers architecture vulnerabilities, plugin security, and compromise vectors.&lt;/p&gt;
&lt;h3 id="agentic-tool-chain-compromise-threats"&gt;&lt;a href="https://www.crowdstrike.com/en-us/blog/how-agentic-tool-chain-attacks-threaten-ai-agent-security/"&gt;Agentic tool chain compromise threats&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Research reveals how attackers exploit AI agent tool chains for code execution and data theft through legitimate workflows. The core insight: agents implicitly trust their tools, creating an exploitable attack surface.&lt;/p&gt;
&lt;hr /&gt;
&lt;h2 id="in-brief"&gt;In brief&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Malicious OpenClaw plugins&lt;/strong&gt;: A supply-chain attack delivered credential stealers disguised as functional plugins, exploiting ecosystem trust.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Ghidra MCP server&lt;/strong&gt;: A developer released 110 tools integrating Ghidra with AI assistants via Model Context Protocol, enabling AI-assisted binary analysis.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Previous roundup: &lt;a href="https://www.accidentalrebel.com/developer-tools-are-the-new-attack-surface.html"&gt;Developer Tools Are the New Attack Surface&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Next roundup: &lt;a href="https://www.accidentalrebel.com/your-ai-assistant-might-be-working-for-someone-else.html"&gt;Your AI Assistant Might Be Working for Someone Else&lt;/a&gt; — Copilot and Grok repurposed as C2 channels.&lt;/p&gt;
&lt;p&gt;For a practical defensive response to agentic attack surfaces, see how I &lt;a href="https://www.accidentalrebel.com/running-ai-agents-in-a-box.html"&gt;run AI agents in an isolated Docker container&lt;/a&gt;.&lt;/p&gt;</content><category term="Cybersecurity x AI"/><category term="ai"/><category term="security"/><category term="cybersecurity-x-ai"/><category term="ai-attack-surface"/><category term="ai-agent-risk"/><category term="ai-threat-intel"/></entry><entry><title>Developer Tools Are the New Attack Surface</title><link href="https://www.accidentalrebel.com/developer-tools-are-the-new-attack-surface.html" rel="alternate"/><published>2026-01-31T10:00:00+08:00</published><updated>2026-01-31T10:00:00+08:00</updated><author><name>AccidentalRebel</name></author><id>tag:www.accidentalrebel.com,2026-01-31:/developer-tools-are-the-new-attack-surface.html</id><summary type="html">&lt;p&gt;VS Code AI extensions with 1.5M installs stealing source code, 175K Ollama servers exposed globally, and AI running autonomous multi-stage network attacks.&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;img alt="Developer Tools Are the New Attack Surface" src="https://www.accidentalrebel.com/images/developer-tools-are-the-new-attack-surface.png" /&gt;
&lt;em&gt;Image made by Gemini&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Developer tools are the new attack surface. VS Code extensions with 1.5 million installs exfiltrating source code to China, 175,000 unsecured Ollama servers globally accessible, and CrowdStrike research on compromising AI agent tool chains. Plus: a former Google engineer convicted for AI trade secret theft, and AI models now conducting autonomous multi-stage network attacks.&lt;/p&gt;
&lt;!-- PELICAN_END_SUMMARY --&gt;

&lt;hr /&gt;
&lt;h2 id="featured-stories"&gt;Featured stories&lt;/h2&gt;
&lt;h3 id="175000-ollama-ai-servers-exposed-without-authentication"&gt;&lt;a href="https://thehackernews.com/2026/01/researchers-find-175000-publicly.html"&gt;175,000 Ollama AI servers exposed without authentication&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;SentinelOne and Censys discovered 175,000 Ollama instances across 130 countries operating without authentication. These self-hosted AI servers, frequently deployed on corporate networks, are publicly accessible. Free compute for attackers, free data for exfiltration. Organizations running Ollama should verify firewall configurations.&lt;/p&gt;
&lt;h3 id="vs-code-ai-extensions-with-15-million-installs-steal-developer-source-code"&gt;&lt;a href="https://thehackernews.com/2026/01/malicious-vs-code-ai-extensions-with-15.html"&gt;VS Code AI extensions with 1.5 million installs steal developer source code&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Two extensions marketed as AI coding assistants reached 1.5 million installations while covertly exfiltrating developer source code to Chinese servers. Despite appearing legitimate and providing functional AI features, they simultaneously harvested typed content. This represents a shift in supply chain attacks from npm packages toward IDE plugins.&lt;/p&gt;
&lt;h3 id="agentic-tool-chain-attacks-turning-ai-agents-against-themselves"&gt;&lt;a href="https://www.crowdstrike.com/en-us/blog/how-agentic-tool-chain-attacks-threaten-ai-agent-security/"&gt;Agentic tool chain attacks: turning AI agents against themselves&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;CrowdStrike documented methods for exploiting AI agent tool chains to achieve code execution and data exfiltration. These attacks leverage legitimate agent workflows by compromising trusted tools rather than agents themselves. The vulnerability stems from implicit agent trust in their tools. Organizations deploying agents with internal system access should review this research.&lt;/p&gt;
&lt;h3 id="ex-google-engineer-convicted-for-stealing-ai-trade-secrets"&gt;&lt;a href="https://thehackernews.com/2026/01/ex-google-engineer-convicted-for.html"&gt;Ex-Google engineer convicted for stealing AI trade secrets&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Linwei Ding received conviction on seven counts of economic espionage and trade secret theft for transferring Google's AI research to a China-based startup. The prosecution underscores that AI intellectual property theft now receives national security treatment, with additional similar cases anticipated.&lt;/p&gt;
&lt;h3 id="ai-models-now-run-autonomous-multi-stage-network-attacks"&gt;&lt;a href="https://www.schneier.com/blog/archives/2026/01/ais-are-getting-better-at-finding-and-exploiting-security-vulnerabilities.html"&gt;AI models now run autonomous multi-stage network attacks&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Bruce Schneier documented AI models executing multi-stage network attacks autonomously using standard penetration testing tools without requiring human guidance between steps. Previously requiring skilled operators, models now independently conduct reconnaissance, exploitation, and data exfiltration. The barrier to sophisticated attacks has significantly lowered.&lt;/p&gt;
&lt;hr /&gt;
&lt;h2 id="in-brief"&gt;In brief&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://thehackernews.com/2026/01/researchers-uncover-chrome-extensions.html"&gt;Chrome extensions harvesting ChatGPT tokens&lt;/a&gt;&lt;/strong&gt;: Malicious browser extensions redirect affiliate links and collect OpenAI authentication tokens from logged-in users. Extension audits recommended.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://thehackernews.com/2026/01/fake-moltbot-ai-coding-assistant-on-vs.html"&gt;Fake AI coding assistant on VS Code delivers malware&lt;/a&gt;&lt;/strong&gt;: An extension named Moltbot posed as free AI assistance while delivering malware. Microsoft removed it, though the pattern persists.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://thehackernews.com/2026/01/konni-hackers-deploy-ai-generated.html"&gt;North Korean Konni group using AI-generated backdoors&lt;/a&gt;&lt;/strong&gt;: Konni threat actors employ AI to generate PowerShell backdoors targeting blockchain developers. State-sponsored actors now use generative AI for malware development.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://www.bleepingcomputer.com/news/security/hugging-face-abused-to-spread-thousands-of-android-malware-variants/"&gt;Hugging Face abused to host Android malware&lt;/a&gt;&lt;/strong&gt;: Attackers distribute thousands of Android malware variants through Hugging Face repositories for credential harvesting. AI platforms are becoming malware distribution infrastructure.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://unit42.paloaltonetworks.com/real-time-malicious-javascript-through-llms/"&gt;LLMs generate phishing JavaScript in real-time&lt;/a&gt;&lt;/strong&gt;: Unit 42 documented attacks where malicious webpages invoke LLM APIs to dynamically generate phishing code in-browser with varying payloads, defeating signature-based detection.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://thehackernews.com/2026/01/who-approved-this-agent-rethinking.html"&gt;AI agent access control is a governance gap&lt;/a&gt;&lt;/strong&gt;: Analysis reveals organizations deploy AI agents without proper access controls or accountability frameworks, leaving agent permissions unmonitored.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href="https://simonwillison.net/2026/Jan/30/moltbook/"&gt;Moltbook shows prompt injection risks in agent networks&lt;/a&gt;&lt;/strong&gt;: Simon Willison examines Moltbook, a social network for AI agent interaction, identifying heightened prompt injection threats when agents communicate with other agents.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Next roundup: &lt;a href="https://www.accidentalrebel.com/ai-agents-under-attack.html"&gt;AI Agents Under Attack&lt;/a&gt; — Claude finds 500+ vulns and LLMs run autonomous network breaches.&lt;/p&gt;
&lt;p&gt;One practical response to these threats: &lt;a href="https://www.accidentalrebel.com/running-ai-agents-in-a-box.html"&gt;Running AI Agents in a Box&lt;/a&gt;, where I containerize AI coding tools with network lockdown.&lt;/p&gt;</content><category term="Cybersecurity x AI"/><category term="ai"/><category term="security"/><category term="cybersecurity-x-ai"/><category term="ai-attack-surface"/><category term="ai-agent-risk"/><category term="ai-threat-intel"/><category term="ai-deployment-security"/></entry></feed>