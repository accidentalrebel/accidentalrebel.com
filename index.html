<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AccidentalRebel.com</title>
    
    <link href="https://www.accidentalrebel.com/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="AccidentalRebel.com Atom Feed" />
    
    <link rel="stylesheet" href="./theme/css/main.css">
    
</head>
<body>
    <div class="progress-bar" id="progress"></div>
    
    <header>
        <div class="container">
            <nav>
                <a href="./" class="site-title">AccidentalRebel.com</a>
                <button class="menu-toggle" onclick="toggleMenu()">☰</button>
                <ul class="nav-links" id="navLinks">
                    <li><a href="./archives.html">Archives</a></li>
                    <li><a href="./categories.html">Categories</a></li>
                    <li><a href="./tags.html">Tags</a></li>
                </ul>
            </nav>
        </div>
    </header>

    <!-- Comic strip divider -->
    <div class="comic-container">
        <div class="comic-strip"></div>
    </div>

    <main>
        <div class="container">
    <article class="post-item">
        <header class="article-header">
            <h2 class="post-title">
                <a href="./running-ai-agents-in-a-box.html">Running AI agents in a box because I don't trust them</a>
            </h2>
            <div class="article-meta">
                <time datetime="2026-01-30T10:00:00+08:00">Fri 30 January 2026</time>
                <span>•</span>
                <a href="./category/programming.html">programming</a>
            </div>
        </header>

        <div class="article-content">
            <p>I built a Docker wrapper for Claude Code and OpenAI Codex. The main reason is simple: I don't trust AI agents running loose on my machine.</p>
<p>Being in Cyber Security, I've developed a healthy paranoia about software that can execute arbitrary commands. AI coding assistants are powerful, but they're also unpredictable. They can run shell commands, modify files, and access the network. I wanted all of that contained.</p>
<h2 id="the-setup">The setup</h2>
<p>Claudecker is my personal tool that wraps Docker to run Claude Code CLI and Codex CLI in an isolated container. Point it at any project directory and it mounts that directory into the container. The AI can do whatever it wants inside the container, but it can't touch the rest of my system.</p>
<div class="codehilite"><pre><span></span><code>./claudecker.sh<span class="w"> </span>run<span class="w"> </span>/path/to/project
</code></pre></div>

<p>Each run starts with a fresh environment. Skills get reinstalled, settings reset to defaults. Only authentication tokens persist across restarts. This "clean slate" approach means I don't accumulate cruft or unexpected state changes.</p>
<h2 id="the-paranoid-feature-network-lockdown">The paranoid feature: network lockdown</h2>
<p>The feature I'm most pleased with is the network lockdown toggle. It uses iptables to control the container's OUTPUT chain policy.</p>
<div class="codehilite"><pre><span></span><code>./claudecker.sh<span class="w"> </span>lockdown
</code></pre></div>

<p>This drops all outbound traffic except localhost and already-established connections. The AI can still work on code, but it can't phone home, download packages, or exfiltrate anything. I toggle this on when working on sensitive projects.</p>
<p>The implementation is straightforward. Just flipping between DROP and ACCEPT policies. The container needs NET_ADMIN capability for this to work, which is a trade-off I'm comfortable with since it's scoped to network operations.</p>
<h2 id="trade-offs-from-containerization">Trade-offs from containerization</h2>
<p>Isolation comes with friction. I had to solve several problems that wouldn't exist if I just ran the CLI directly on my host.</p>
<h3 id="browser-authentication-needs-x11">Browser authentication needs X11</h3>
<p>Claude and Codex authentication use browser-based OAuth flows. Inside a container, there's no browser. I ended up mounting the X11 socket and forwarding the DISPLAY variable:</p>
<div class="codehilite"><pre><span></span><code><span class="nt">volumes</span><span class="p">:</span>
<span class="w">  </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">/tmp/.X11-unix:/tmp/.X11-unix:rw</span>
</code></pre></div>

<p>On Linux this works if you have DISPLAY set. On macOS you need XQuartz. For headless environments, there's a fallback where I manually copy the auth.json file from a machine where you've already logged in.</p>
<p>Thankfully, Claude Code and Codex CLI makes it easier by providing you a URL to visit, which will give you a code to enter back. This means I rarely need to use browser authentication, but at least the option is there.</p>
<h3 id="ssh-agent-forwarding-is-annoying">SSH agent forwarding is annoying</h3>
<p>Getting SSH keys into the container without copying them required some workarounds. Docker's socket permissions don't always cooperate, so I ended up using socat to proxy the SSH agent socket:</p>
<div class="codehilite"><pre><span></span><code>sudo<span class="w"> </span>socat<span class="w"> </span>UNIX-LISTEN:/tmp/ssh-agent-forwarded,fork,mode<span class="o">=</span><span class="m">600</span>,user<span class="o">=</span>node<span class="w"> </span><span class="se">\</span>
<span class="w">          </span>UNIX-CONNECT:/ssh-agent<span class="w"> </span><span class="p">&amp;</span>
</code></pre></div>

<p>The container tries direct socket access first, falls back to socat if that fails. Limited sudo permissions ensure the node user can only run specific commands.</p>
<h3 id="port-forwarding-for-web-apps">Port forwarding for web apps</h3>
<p>If you're developing a web app and want to access it from your host browser, you need to expose ports explicitly:</p>
<div class="codehilite"><pre><span></span><code>./claudecker.sh<span class="w"> </span>run<span class="w"> </span>--port<span class="w"> </span><span class="m">3000</span><span class="w"> </span>/path/to/project
</code></pre></div>

<p>This maps the container's port to the host. Without this flag, localhost:3000 inside the container isn't reachable from outside.</p>
<h2 id="project-specific-dependencies">Project-specific dependencies</h2>
<p>Different projects need different tools. A C project needs gcc and cmake. A Python ML project needs different libraries. I didn't want to bloat the base image with everything.</p>
<p>The solution: a <code>.claudecker</code> file in the project directory.</p>
<div class="codehilite"><pre><span></span><code># .claudecker
build-essential
cmake
gcc
python3-dev
</code></pre></div>

<p>On first run, the script hashes the file contents, generates a Dockerfile, and builds a custom image tagged with that hash. Subsequent runs use the cached image. Projects with identical <code>.claudecker</code> files share the same image.</p>
<div class="codehilite"><pre><span></span><code>claudecker-custom:a1b2c3d4e5f6
</code></pre></div>

<p>This content-based approach means I'm not rebuilding images unnecessarily, and cleanup is straightforward with <code>clean-custom</code> and <code>clean-all-custom</code> commands.</p>
<h2 id="skills-system">Skills system</h2>
<p>A new and recent addition: Claudecker now supports Claude Skills, which are custom prompts that extend its capabilities. I implemented two types:</p>
<ul>
<li>GitHub skills get cloned on container startup. The <a href="https://github.com/blader/humanizer">Humanizer skill</a>, for example, comes from a public repo and helps remove AI-isms from text.</li>
<li>Local skills are baked into the Docker image. I keep these in a <code>local-skills/</code> directory.</li>
</ul>
<p>The build process copies these into the image, and the entrypoint installs them into Claude's skills directory. This way I can version-control project-specific skills alongside the code.</p>
<h2 id="multi-ai-orchestration-with-pal-mcp">Multi-AI orchestration with PAL MCP</h2>
<p>I also integrated PAL MCP Server, which lets Claude Code collaborate with other AI models (Gemini, GPT, Grok, local Ollama models). I export my API keys before running:</p>
<div class="codehilite"><pre><span></span><code><span class="nb">export</span><span class="w"> </span><span class="nv">OPENROUTER_API_KEY</span><span class="o">=</span><span class="s2">&quot;your-key&quot;</span>
./claudecker.sh<span class="w"> </span>run<span class="w"> </span>/path/to/project
</code></pre></div>

<p>Inside Claude Code, I can ask it to use other models for second opinions, code review, or extended reasoning. The MCP server handles the routing.</p>
<p>This obviously requires network access, so it doesn't work in lockdown mode. Trade-offs.</p>
<h2 id="wheres-the-code">Where's the code?</h2>
<p>I planned to release this publicly but decided against it for now. There are rough edges:</p>
<ul>
<li>The docker-compose volumes are duplicated in the docker run command for custom images. If you change one, you have to change the other. I left a warning comment but it's still error-prone.</li>
<li>The firewall whitelist script exists but isn't fully tested across different network configurations.</li>
<li>Some features assume specific host setups (X11, SSH agent running, etc.) and fail ungracefully when those assumptions don't hold.</li>
<li>Error handling is minimal in places.</li>
</ul>
<p>I use this daily for my own work, but it's not polished enough for others to pick up without reading through the scripts first. Maybe later.</p>
<h2 id="current-limitations">Current limitations</h2>
<ul>
<li>Network lockdown state doesn't persist across container restarts. Restart the container and you're back to full network access.</li>
<li>Custom image builds happen automatically but failures silently fall back to the base image. You might not notice a package didn't install.</li>
<li>X11 forwarding is a security surface I'm not entirely comfortable with, but I haven't found a better solution for browser auth.</li>
</ul>
<h2 id="what-i-actually-use-it-for">What I actually use it for</h2>
<p>Most days I run Claude Code in lockdown mode for general coding tasks. When I need it to fetch documentation or install packages, I toggle lockdown off, let it do its thing, then toggle it back on.</p>
<p>For security research projects, the isolation gives me peace of mind. The AI can analyze suspicious code, suggest modifications, even run tests, all without access to my actual filesystem or network.</p>
<p>It's not perfect containment. Docker isn't a security boundary the way a VM is. But it's enough friction that an AI agent can't accidentally (or intentionally) do something I'd regret.</p>
<p>For now, this setup works for my needs. The paranoia tax is a few extra seconds on startup and occasional friction with browser auth. Worth it.</p>
        </div>

        <div class="tags">
            <a href="./tag/docker.html" class="tag">docker</a>
            <a href="./tag/ai.html" class="tag">ai</a>
            <a href="./tag/claude-code.html" class="tag">claude-code</a>
            <span class="tag">+2 more</span>
        </div>
    </article>
    <article class="post-item">
        <header class="article-header">
            <h2 class="post-title">
                <a href="./classifying-more-with-less-new-vgl4nt-update.html">Classifying More With Less: New VGL4NT Update</a>
            </h2>
            <div class="article-meta">
                <time datetime="2023-05-20T06:46:00+08:00">Sat 20 May 2023</time>
                <span>•</span>
                <a href="./category/misc.html">misc</a>
            </div>
        </header>

        <div class="article-content">
            <h2 id="tldr">TLDR:</h2>
<ul>
<li>Packed malware machine learning classifier can only previously identify 10 packers</li>
<li>Solution was a customized version of model ensembling, which is to train multiple models and resolve their results</li>
<li>It works with a slight caveat of more extended training and processing, which I could happily live with</li>
</ul>
<p>I recently presented <a href="https://packers.vgl4nt.com/">VGL4NT</a>, my tool that uses machine learning to classify packed malware, at the Blackhat Middle East and Africa meetup. During my talk, I candidly shared one of the tool's limitations which is it can only identify 10 packers because of my hardware constraints. If I want it to be able to identify more, I need to get more GPU (which will be costly) or keep my money and come up with a clever solution. Well, this post is about the latter.</p>
<h2 id="a-simple-solution">A Simple Solution</h2>
<p>The solution I came up with isn't exactly original. It's based on Task Decomposition, which involves training separate models for different categories and combining their predictions. This way, I could double the classification capacity without requiring additional hardware resources.</p>
<p>This was implemented by creating multiple machine learning models, each specializing in recognizing a subset of packers. The real challenge, however, lies in combining the predictions from these models to form a unified output.</p>
<p><img alt="classifying-more-with-less-new-vgl4nt-update-01" src="./images/classifying-more-with-less-new-vgl4nt-update-01.png" /></p>
<p>Here's how the process works:</p>
<p>The packed malware file is fed into Model 1, which outputs probabilities for Packer 1, Packer 2, and Others. For example, it might produce:</p>
<ul>
<li>Packer 1: 10%</li>
<li>Packer 2: 20%</li>
<li>Others: 70%</li>
</ul>
<p>The same file is then fed into Model 2, which outputs probabilities for Packer 3, Packer 4, and Others. For instance:</p>
<ul>
<li>Packer 3: 60%</li>
<li>Packer 4: 30%</li>
<li>Others: 10%</li>
</ul>
<p>I then take the 'Others' category with the lowest probability. For our example, the final 'Others' probability would be 10% from Model 2.</p>
<p>The final probabilities are:</p>
<ul>
<li>Packer 1: 10%</li>
<li>Packer 2: 20%</li>
<li>Packer 3: 60%</li>
<li>Packer 4: 30%</li>
<li>Others: 10%</li>
</ul>
<p>Packer 3 has the highest probability in this example, and the file is classified as such.</p>
<p>This simple combination approach ensures I maintain a suitable probability distribution while leveraging each model's strengths. The beauty of this method is not only its efficiency but also its scalability. I can introduce more models, each specializing in different packers, to further increase the classification capabilities.</p>
<p>Now you might wonder why I'd even write about this if the solution is this simple. The funny thing is I've explored multiple approaches to unifying the output. Before this, I fully implemented a complicated approach, only to later realize while writing this blog post that a much simpler approach works well enough for the tool's purpose.</p>
<h2 id="downsides">Downsides</h2>
<p>I am conscious that this may or may not be the most effective method to tackle this problem. But what is essential is that the current computation is simple and can maintain the appropriate prediction distribution based on the relative percentages. In essence, the category with the highest confidence score will always come out on top in the final output, primarily what users of my tool are interested in.</p>
<p>Aside from this, I am concerned that increasing the number of categories also increases training and prediction time. I'm not too worried about the increase in training time because this happens behind the scenes and remains unseen to users of my tool. I'm slightly concerned about the longer prediction time, as all models need to process each submission to the tool. And as I plan to incorporate more packer tool categories, the prediction time will definitely rise.</p>
<p>These downsides are not too much of a problem, however. They can easily be fixed if I find they are not meeting the tool's goals. For now, these will do.</p>
<h2 id="conclusion">Conclusion</h2>
<p>I am genuinely happy with my progress with the VGL4NT Malware Packer Classifier. There are other topics I want to tackle, but I'll save those for future blog posts.</p>
<p>In the meantime, I invite you to check out the tool and see the changes yourself. Visit the <a href="https://packers.vgl4nt.com/">VGL4NT website</a> to get started. And for a more detailed walkthrough, you can also watch this <a href="https://www.youtube.com/watch?v=emIyy4Njw_g">YouTube video</a> I created</p>
        </div>

        <div class="tags">
            <a href="./tag/packers-malware-machine_learning-ml-update-vgl4nt.html" class="tag">packers malware machine_learning ml update vgl4nt</a>
        </div>
    </article>
    <article class="post-item">
        <header class="article-header">
            <h2 class="post-title">
                <a href="./classifying-malware-packers-using-machine-learning.html">Classifying Malware Packers Using Machine Learning</a>
            </h2>
            <div class="article-meta">
                <time datetime="2023-04-22T08:44:00+08:00">Sat 22 April 2023</time>
                <span>•</span>
                <a href="./category/misc.html">misc</a>
            </div>
        </header>

        <div class="article-content">
            <p>The recent rise in popularity of AI reignited my interest in machine learning. It inspired me to dive deeper into understanding how it can be applied to malware analysis and, more importantly, how to better detect malware packers, as almost every malware nowadays uses them.</p>
<p>My research and experiments eventually led me to make a web app, which I call the VGL4NT Malware Packer Classifier (<a href="https://packers.vgl4nt.com/).">https://packers.vgl4nt.com/).</a>).</p>
<p><img alt="classifying-malware-packers-using-machine-learning-01" src="./images/classifying-malware-packers-using-machine-learning-01.png" /></p>
<p>(For those curious, V.G.L.4.N.T. is a play on "Vigilant" and stands for "Visual Guided Learning 4 Neutralizing Threats")</p>
<h1 id="current-state-of-packer-detection">Current State of Packer Detection</h1>
<p>Traditional packer detection approaches like DiE (Detect it Easy) and Yara rules depend on known signatures and patterns to identify packers. These tools scrutinize a file for specific indicators, like unique sequences of bytes or strings. While effective in many cases, they have drawbacks, like when a packer is modified or if the sequence of bytes or strings are altered.</p>
<p>By using machine learning, the VGL4NT Malware Packer Classifier can be able to take into account minute differences and still be able to detect the packer used.</p>
<h1 id="how-it-works">How it works</h1>
<ul>
<li>The uploaded executable file's bytes are converted into grayscale values, creating an image..</li>
<li>The grayscale image is then fed into an image machine-learning model I trained from scratch.</li>
<li>It returns a list of percentages on how similar it is to other Packers.</li>
</ul>
<p><img alt="classifying-malware-packers-using-machine-learning-01" src="./images/classifying-malware-packers-using-machine-learning-02.png" /></p>
<p>The approach above is nothing new and is based on <a href="https://ieeexplore.ieee.org/abstract/document/8328749">this academic paper</a>. The difference is that the paper has a tool that classifies malware families, while mine classifies the packers used.</p>
<p>Most of the magic happens in the model itself. I've trained it on several packed malware samples and measured its accuracy using multiple iterations. The latest version of this model has a 94% accuracy, which is calculated by comparing the model's predictions to the actual packer labels in a dataset that the model hasn't seen before (the test dataset).</p>
<h1 id="current-limitations">Current limitations</h1>
<p>The app works for the most part, but it has its limitations. For example, users can only upload executable files (EXE, Bin, ELF, DLLs, etc) with a maximum size limit of 10MB. </p>
<p>Furthermore, due to costs of GPU resources during training, only the following packer tools can be classified:</p>
<ul>
<li>aspack</li>
<li>alienyze</li>
<li>amber</li>
<li>mew</li>
<li>mpress</li>
<li>nspack</li>
<li>pecompact</li>
<li>petite</li>
<li>themida</li>
<li>upx</li>
<li>others (Everything else)</li>
</ul>
<p>The list of packer tools above was chosen based on available real-world malware samples that I have encountered or studied.</p>
<h1 id="future-plans-and-updates">Future Plans and Updates</h1>
<p>If this project gains enough interest, then I plan to add more improvements, such as:</p>
<ul>
<li>Increase GPU resources to increase the model's capacity to classify more categories</li>
<li>Improvements in the training method by handpicking the most important parts of the executable and then feeding that to the model</li>
<li>Offer an API for integration with existing tools and processes.</li>
</ul>
<p>Of course, this project would improve a lot with the community's help. I encourage users to provide feedback, report issues, or request new features. Feel free to throw your thoughts to me through my email, karlo@accidentalrebel.com, or Twitter at @accidentalrebel.</p>
        </div>

        <div class="tags">
            <a href="./tag/artificial_intelligence.html" class="tag">artificial_intelligence</a>
            <a href="./tag/machine_learning.html" class="tag">machine_learning</a>
        </div>
    </article>

<nav class="pagination">

        <span class="current">1</span>
        <a href="./index2.html">2</a>
        <a href="./index3.html">3</a>
        <a href="./index4.html">4</a>
        <a href="./index5.html">5</a>
        <a href="./index6.html">6</a>
        <a href="./index7.html">7</a>
        <a href="./index8.html">8</a>
        <a href="./index9.html">9</a>
        <a href="./index10.html">10</a>
        <a href="./index11.html">11</a>
        <a href="./index12.html">12</a>
        <a href="./index13.html">13</a>
        <a href="./index14.html">14</a>
        <a href="./index15.html">15</a>
        <a href="./index16.html">16</a>

    <a href="./index2.html">Next &raquo;</a>
</nav>
        </div>
    </main>

    <script src="./theme/js/main.js"></script>
    
    <!-- Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-55068085-2"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'UA-55068085-2');
    </script>
</body>
</html>